---
title: "Proyecto predicción de precios en viviendas"
output: html_notebook
---

Importación y carga de paquetes

```{r}
library(tidyverse)
library(caret)
library(data.table)
library(ggplot2)
library(dplyr)
library(corrplot)
library(psych)
library(randomForest)
library(Metrics)

set.seed(1)
```


Carga de los datos (Tarea 0)

```{r}
train_df <- read_csv('./data/train.csv')
test_df <- read_csv('./data/test.csv')

```

#Visualización de los datos

Vamos a observar las primeras entradas de nuestro conjunto de datos para conocer un poco más de información sobre
el dataset.

```{r}
train_df %>% head()
```

Número de filas y columnas del conjunto de entrenamiento.
```{r}
nrow(train_df)
ncol(train_df)
```

Número de filas y columnas del conjunto de prueba.
```{r}
nrow(test_df)
ncol(test_df)
```

```{r}
summary(train_df)
```

Distribución de los precios de las casas. Encontramos que la gran mayoria se encuentra en el rango 100000-20000

```{r}
ggplot(train_df, aes(x=SalePrice)) + 
    geom_histogram(color="black",fill="blue",bins=50) + 
    scale_x_continuous(name="Precio de venta", breaks=seq(0,800000,100000)) + 
    scale_y_continuous(name="Número de casas") + 
    theme(axis.text.x = element_text(angle=45,size=7))
```

Visualización de atributos con valor NA

```{r}
missing_values <- train_df %>%
    gather(key = "key", value = "val") %>%
    mutate(is.missing = is.na(val)) %>%
    group_by(key, is.missing) %>%
    summarise(num.missing = n()) %>%
    filter(is.missing==T) %>%
    select(-is.missing) %>%
    arrange(desc(num.missing)) 

missing_values %>%
  ggplot() +
    geom_bar(color="black",fill="blue",aes(x=key, y=num.missing), stat = 'identity') +
    labs(x='Atributo', y="Número de valores NA") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# Preprocesamiento.

# Eliminación de atributos con más de un 10% de entradas con valor NA.

Primero obtendremos una tabla que nos muestre cuantos valores NA existen para cada columna.
```{r}
na_count <- sapply(train_df, function(y) sum((is.na(y))))
na_count <- data.frame(na_count)

missing_values

```

Ahora procederemos a eliminar las columnas que tengan más de un 10% de entradas con valor NA

```{r}
columns_na <- na_count %>% filter((na_count/nrow(train_df)*100) > 10) %>% rownames()
columns_na
```

Por último, eliminaremos estas columnas tanto de nuestro conjunto de entrenamiento como en el de prueba.

```{r}
train_df <- train_df[,!(names(train_df) %in% columns_na)]
test_df <- test_df[,!(names(test_df) %in% columns_na)]
```
# Rellenar campos con valor NA.

Para todos los valores NA restantes, los modificaremos por las valores más frecuentes o con la media.

```{r}
na_modified <- function(x) {
  if (is.numeric(x)) {
    x[is.na(x)] <- mean(x, na.rm = TRUE)
    x
  } else {
    x[is.na(x)] <- names(which.max(table(x)))
    x
  }
}

train_df <- as.data.frame(lapply(train_df,na_modified))
test_df <- as.data.frame(lapply(test_df,na_modified))
```

# Eliminación de variables no relevantes mediante correlación (Tarea 12)

# NUMÉRICAS

```{r}
train_df_nums <- select_if(train_df, is.numeric)
train_df_not_nums <- select_if(train_df, negate(is.numeric))
test_df_nums <- select_if(test_df, is.numeric)
test_df_not_nums <- select_if(test_df, negate(is.numeric))
```

```{r}

train_cor <- cor(train_df_nums[ , colnames(train_df_nums) != "SalePrice"],
                train_df_nums$SalePrice)
```

```{r}
corrplot(train_cor,
         cl.pos='n',
         insig = 'p-value')
```
```{r}
cor_coef_df <- as.data.frame(train_cor) %>% 
    rename('abs_cor_coef' = V1) %>%
    mutate(abs_cor_coef = abs(abs_cor_coef))
cor_coef_df
```
```{r}
drop_columns_less_20 <- cor_coef_df   %>% filter(abs_cor_coef <0.2)
drop_columns_less_20
```
```{r}
#bigger_threshold <- cor_coef_df > 0.2
bigger_threshold <- replace(cor_coef_df, cor_coef_df<0.2, "red")
bigger_threshold <- replace(bigger_threshold, cor_coef_df>0.2, "green")
bigger_threshold[,1]
```

```{r}
col_names <- row.names(cor_coef_df)
```

```{r}
df <- as.data.frame(cor_coef_df)
```


```{r}
ggplot(df
       , aes(x=col_names, y=abs_cor_coef, fill = col_names)
       ) + 
  scale_fill_manual(
    breaks = c(col_names),
    values=c(bigger_threshold[,1])) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 70, vjust = 0.85, hjust=1))
```
```{r}
train_df

```

```{r}
row_names_less_20 <- (row.names(drop_columns_less_20))
row_names_less_20 <- row_names_less_20[row_names_less_20 != "Id"]
train_df <- train_df %>% select(-row_names_less_20)
test_df <- test_df %>% select(-row_names_less_20)
```


Eliminación de variables que no aportan información al algoritmo (Tarea 5)

Comprobamos cuántos valores únicos tiene cada columna de tipo string.


```{r}
unique_count <- as.data.frame(sapply(train_df_not_nums, function(x) length(unique(x)))) %>% 
   rename('value' = 'sapply(train_df_not_nums, function(x) length(unique(x)))')

uniques_col_names <- row.names(unique_count)
threshold <- 8
uniques_threshold <- replace(unique_count, unique_count>=threshold, "red")
uniques_threshold <- replace(uniques_threshold, uniques_threshold<threshold, "green")

ggplot(unique_count
       , aes(x=uniques_col_names, y=value, fill=uniques_col_names)
       ) + 
  scale_fill_manual(
    breaks = c(uniques_col_names),
    values=c(uniques_threshold[,1])) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 70, vjust = 0.85, hjust=1))
```
```{r}
more_than_threshold <- unique_count %>% filter(value >= threshold ) %>% rownames()
more_than_threshold
```
```{r}
# Esto debería hacerse con train y test, pero parece que no tienen las mismas dimensiones, por eso lo hago con las variables categóricas solamente
test_df <- test_df %>% mutate(SalePrice = -1);
merged_df <- rbind(train_df, test_df) %>% select(-more_than_threshold)

dummy_vars <- dummyVars(" ~ .", data = merged_df)
merged_df <- data.frame(predict(dummy_vars, newdata = merged_df))

# Separar train y test, para cuando se puedan utilizar con la variable id
train_df <- merged_df %>% filter(Id < 1461)
test_df <- merged_df %>% filter(Id > 1460) %>% select(-SalePrice)

#Como hemos creado muchas columnas vamos a utilizar RFE(Random Feature Elimination) para reducir la dimensionalidad
sales_prices <- train_df$SalePrice
train_df <- train_df %>% select(-c("Id", "SalePrice"))
test_df <- test_df %>% select(-Id)

#Aplicamos escalado estándar para RFE
train_df <- scale(train_df)
test_df <- scale(test_df)                              

#Crear train y test para RFE
data_partition <- createDataPartition(sales_prices, p = .70, list = FALSE)
RFE_X_train <- train_df[data_partition,]
RFE_X_test <- train_df[-data_partition,]
RFE_Y_train <- sales_prices[data_partition]
RFE_Y_test <- sales_prices[-data_partition]

num_variables <- c(80, 100, 120, 140)
control <- rfeControl(functions = rfFuncs, method = "repeatedcv", repeats = 3, number = 5)
rfe <- rfe(x = RFE_X_train, y = RFE_Y_train, sizes = num_variables, rfeControl = control)
ggplot(rfe)
```
```{r}
columns <- predictors(rfe)

train_df <- as.data.frame(train_df)
test_df <- as.data.frame(test_df)

sales_prices_df <- as.data.frame(sales_prices) %>% rename(SalePrice = sales_prices)
train_df <- cbind(train_df, sales_prices_df) %>% select(append(columns, 'SalePrice'))
test_df <- test_df %>% select(columns)
```

```{r}
train_df_training <- train_df %>% select(-SalePrice)
```

# Extreme Gradient Boosting (XGB)

```{r}
xgb_grid = expand.grid(nrounds = c(1000,2000, 3000), #Número de iteraciones. A mayor número mejores resultados pero entrenamiento más lento.
                            eta = c(0.1, 0.01, 0.001), #Tasa de aprendizaje. A mayor valor mejor modelo, pero puede pasarse de su valor óptimo.
                            lambda = 1,
                            alpha = 0)
xgb_trcontrol = trainControl(method = "cv",
                                number = 3,
                                verboseIter = TRUE,
                                returnData = FALSE,
                                returnResamp = "all", 
                                allowParallel = TRUE)

xgb_model <- train(x = as.matrix(train_df_training),
                    y = sales_prices_df$SalePrice,
                    trControl = xgb_trcontrol,
                    tuneGrid = xgb_grid,
                    method = "xgbLinear",
                    metric= "RMSLE",
                    max.depth = 5) #Profundidad de los árboles de regresión. Si es un valor grande puede haber overfitting.

test_pred = predict(xgb_model , test_df)
train_pred = predict(xgb_model, train_df)
```

# Support Vector Machine (SVM)

```{r}
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
```

```{r}
svm_Linear <- train(x=train_df_training, y=train_df$SalePrice, method = "svmLinear",
    trControl=trctrl,
#    preProcess = c("center", "scale"),
    tuneLength = 10)
```

```{r}
svm_Linear
```

```{r}
test_pred <- predict(svm_Linear, newdata = test_df)
test_pred
```


```{r}
svm_radial <- train(x=train_df_training, y=train_df$SalePrice, method = "svmRadial",
    trControl=trctrl,
    preProcess = c("center", "scale"),
    tuneLength = 10)
```

```{r}
svm_radial
```

```{r}
test_pred <- predict(svm_radial, newdata = test_df)
rmsle(test_df, test_pred)
```

```{r}
svm_poly <- train(x=train_df_training, y=train_df$SalePrice, method = "svmPoly",
    trControl=trctrl,
    preProcess = c("center", "scale"),
    tuneLength = 4)
```

```{r}
svm_poly
```

```{r}
test_pred_aux <- as.data.frame(test_pred)
test_pred_aux$Id <- rownames(test_df)
```

```{r}
rf_fit <- train(SalePrice ~ ., data = train_df, method = 'rf', importance = TRUE, trControl = trainControl(method = 'cv', number = 10))

rf_fit
```